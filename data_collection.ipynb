{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your overview here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Mariam Issa\n",
    "- Andrea Sudharta\n",
    "- Payam Sadeghian\n",
    "- Brandon Amaral\n",
    "- Alex Luo\n",
    "- Jun Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members IDs\n",
    "\n",
    "- A12285140\n",
    "- A14497101\n",
    "- A13654507\n",
    "- A########\n",
    "- A########\n",
    "- A15743932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your research question here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your background and prior work here* \n",
    "\n",
    "References (include links):\n",
    "- 1)\n",
    "- 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your hypotheses here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your dataset information here*\n",
    "\n",
    "(Copy this information for each dataset)\n",
    "- Dataset Name:\n",
    "- Link to the dataset:\n",
    "- Number of observations:\n",
    "\n",
    "1-2 sentences describing each dataset. \n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeters_dem_df = pd.read_csv('popular-tweeters-dem.csv') #Dataframe for list of Democratic Party politicians\n",
    "tweeters_rep_df = pd.read_csv('popular-tweeters-rep.csv') #Dataframe for list of Republican Party politicians\n",
    "tweeters_dem_df['Party'] = 'D'\n",
    "tweeters_rep_df['Party'] = 'R'\n",
    "tweeters_df = pd.concat([tweeters_dem_df, tweeters_rep_df], ignore_index = True) # Dataframe for combined list of politicians\n",
    "\n",
    "tweeters_handle = list(tweeters_df['Twitter_Handle']) #Python list for politician Twitter handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OAuth from Twitter\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'bSZRBubFkHWewMi08ltv7DgAu'\n",
    "consumer_secret = '6JFpZu66GT7OvoJFFxSfJJlX21NH5wxmMjNDbsQGPLyW9WtBSU'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "try:\n",
    "    redirect_url = auth.get_authorization_url()\n",
    "    print (redirect_url)\n",
    "    \n",
    "except tweepy.TweepError:\n",
    "    print(\"Error! Failed to get request token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Twitter account usage\n",
    "verifier = raw_input('Verifier:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Access Token\n",
    "try: \n",
    "    auth.get_access_token(verifier)\n",
    "except tweepy.TweepError:\n",
    "    print (\"Error! Failed to get access token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokens\n",
    "access_token = auth.access_token\n",
    "acess_token_secret = auth.access_token_secret\n",
    "\n",
    "# Note: You do not need to re-fetch it each time. Twitter currently does\n",
    "# not expire the tokens, so the only time it would ever go invalid is if\n",
    "# the user revokes our application access. \n",
    "\n",
    "#To rebuild an OAuthHandler from the stored access token\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(key, secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Set Up\n",
    "api = tweepy.API(auth)\n",
    "#api.update_status('tweepy + oauth!') # Posts directly on my timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tweets to only include those containing key words\n",
    "key_words_by_bill = {\n",
    "    0:['#savetheinternet', '#netneutrality', 'internet', 'neutrality'], \n",
    "    1:['#medicareforall', '#universalhealthcare','#healthcare', 'health', 'care', 'health', 'care', 'medicare'], \n",
    "    2: ['#equalpay', '#equalrights', '#equalrightsammendment', '#era', '#women', '#genderinequality', '#genderequality', 'equal', 'rights', 'pay', 'women', 'gender', 'equality'],\n",
    "    3:['#votingrights', '#votingrightact', '#electionintegrity', '#riggedelections', '#forthepeople', 'voting', 'rights', 'elections', 'fair', 'vote', 'votes'], \n",
    "    4:['#guncontrol', '#2ndammendment', '#noguncontol', '#gunviolence', '#gunrights', '#gunreform', '#2a', 'gun', 'second', '2nd',  'amendment'], \n",
    "    5:['#yemen', '#yemengenocide', 'yemen', 'yemeni', 'war', 'troops', 'withdraw troops', 'civil'], \n",
    "    6:['#women', '#reauthorizationact', '#violenceagainstwomen', '#genderbasedviolence', '#sexualviolence', 'women'],  \n",
    "    7:['#buildthewall', '#buildthewallnow', '#nationalsecurity', '#nationalemergency', '#trumpswall', '#illegalimmigration', '#mexico', '#border', '#bordercrises', 'mexico', 'illegal', 'immigration', 'security', 'emergency', 'build', 'wall']\n",
    "}\n",
    "\n",
    "key_words = list({x for v in key_words_by_bill.itervalues() for x in v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Intersection Function\n",
    "def interSection(arr1,arr2): \n",
    "    return list(filter(lambda x: x in arr1, arr2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tweets that deal with each bill in Congress\n",
    "bill = [{}] * 8 # bill[bill_number] = {congressman: [tweets about bill]}\n",
    "tweets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_title = {\n",
    "    0: \"Save the Internet\",\n",
    "    1: \"Health Care\",\n",
    "    2: \"Paycheck Fairness Act\",\n",
    "    3: \"For the People Act 2019\",\n",
    "    4: \"Background Check for Firearms\",\n",
    "    5: \"Remove Troops from Yemen\",\n",
    "    6: \"Violence against Women Reauthorization Act\",\n",
    "    7: \"Trump's National Emergency\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through Congressmen\n",
    "for handle in tweeters_handle:\n",
    "    all_tweets = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            tweets_from_page_i = api.user_timeline(handle, count = 200, page = i) # max count value is 200\n",
    "        except:\n",
    "            print(\"TweepError: stopped at handle: \", handle, \" at page \", i)\n",
    "        \n",
    "        # Check if we reached end of Twitter history\n",
    "        if len(tweets_from_page_i) == 0: \n",
    "            break\n",
    "            \n",
    "        #iterate through each tweet, sort by bill\n",
    "        for tweet in tweets_from_page_i:\n",
    "            tweet_tokens = (tweet.text).lower().split()\n",
    "            \n",
    "            # See if Tweet talks about bill matter\n",
    "            for bill_number, bill_tokens in key_words_by_bill.items():\n",
    "                if len(interSection(tweet_tokens, bill_tokens)) != 0:\n",
    "                    if bill[bill_number] == {}:\n",
    "                        bill[bill_number] = {handle:[tweet.text]}\n",
    "                    else:\n",
    "                        if handle not in bill[bill_number]:\n",
    "                            bill[bill_number][handle] = [tweet.text]\n",
    "                        else:\n",
    "                            bill[bill_number][handle] += [tweet.text]\n",
    "                    \n",
    "                    # Compile to all tweets list\n",
    "                    all_tweets.append(tweet.text)\n",
    "                    \n",
    "    # Collect congressman/woman's tweets         \n",
    "    tweets[handle] = all_tweets\n",
    "        \n",
    "    # Sleep: to not overload Twitter requests\n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create look up table for Congressman/woman's name and party\n",
    "tweeters_party = list(tweeters_df['Party'])\n",
    "tweeters_name = list(tweeters_df['Name'])\n",
    "\n",
    "handle_to_party = {}\n",
    "handle_to_name = {}\n",
    "for i, handle in enumerate(tweeters_handle):\n",
    "    handle_to_party[handle] = tweeters_party[i]\n",
    "    handle_to_name[handle] = tweeters_name[i]\n",
    "\n",
    "data = [] \n",
    "\n",
    "# List all tweets by bill it's associated with and the congressman who tweeted it\n",
    "for i in range(len(bill)):\n",
    "    for handle, tweet in bill[i].items():\n",
    "        data.append([handle_to_name[handle], handle_to_party[handle], handle, bill_title[i], tweet])\n",
    "        \n",
    "tweeters_df = pd.DataFrame(data, columns = ['Name', 'Party', 'Twitter Handle', 'Bill #', 'Tweet'])\n",
    "\n",
    "# Export CSV with Tweets\n",
    "tweeters_df.to_csv('tweets_by_bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cloud",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-36eb829dfce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named cloud"
     ]
    }
   ],
   "source": [
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"C:/Users/andrea/OneDrive/Documents/git/truther-tweet//politician-tweet-sentiment-4910a807f774.json\"\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# The text to analyze\n",
    "text = u'Hello, world!'\n",
    "document = types.Document(content=text, type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "# Detects the sentiment of the text\n",
    "sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "\n",
    "print('Text: {}'.format(text))\n",
    "print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a string\n",
    "def sentimentAnalysis(text):\n",
    "    document = types.Document(content=text, type=enums.Document.Type.PLAIN_TEXT)\n",
    "    \n",
    "    # Detects the sentiment of the text\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    \n",
    "    return sentiment.score, sentiment.magnitude\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5dc5ef4f996c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Evaluate by bill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtweet_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbill_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Add list of scores to Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-24c00f4dc87a>\u001b[0m in \u001b[0;36mbill_sentiment\u001b[0;34m(tweet, bill)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentimentAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fe4117e302d2>\u001b[0m in \u001b[0;36msentimentAnalysis\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pass in a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentimentAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPLAIN_TEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Detects the sentiment of the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'types' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweets_df = pd.read_csv('tweets_by_bill')\n",
    "\n",
    "all_sentiment_scores = []\n",
    "avg_scores = []\n",
    "mdn_scores = []\n",
    "rng_scores = [] \n",
    "var_scores = [] \n",
    "std_dev_scores = []\n",
    "\n",
    "for bill, tweets in zip(tweets_df['Bill #'], tweets_df['Tweet']):\n",
    "    \n",
    "    tweet_scores = []\n",
    "    \n",
    "    for twt in tweets.split('u\\''): \n",
    "        # Clean tweet\n",
    "        twt = ' '.join([word.lower() for word in twt.split(' ') if word.isalpha()])\n",
    "        \n",
    "        # Skip empty strings\n",
    "        if twt == '': continue \n",
    "            \n",
    "        # Evaluate by bill\n",
    "        tweet_scores.append(bill_sentiment(twt, bill))\n",
    "        \n",
    "    # Add list of scores to Dataframe\n",
    "    all_sentiment_scores.append(tweet_scores)\n",
    "    \n",
    "    # Get AVERAGE tweet sentiment score\n",
    "    avg = sum(tweet_scores) * 1.0 / len(tweet_scores)\n",
    "    avg_scores.append(avg)\n",
    "        \n",
    "    # Get MEDIAN tweet sentiment score\n",
    "    sorted_tweet_scores = sorted(tweet_scores)\n",
    "    mdn = sorted_tweet_scores[len(tweet_scores) / 2]\n",
    "    mdn_scores.append(mdn)\n",
    "        \n",
    "    # Get RANGE tweet sentiment score\n",
    "    rng = sorted_tweet_scores[len(tweet_scores) - 1] - sorted_tweet_scores[0]\n",
    "    rng_scores.append(rng)\n",
    "        \n",
    "    # Get VARIANCE tweet sentiment score\n",
    "    var = sum([(score - avg)**2 for score in tweet_scores]) / (len(tweet_scores) - 1)\n",
    "    var_scores.append(var)\n",
    "    \n",
    "    # Get STD DEVIATION tweet sentiment score\n",
    "    std_dev = var**(1/2)\n",
    "    std_dev_scores.append(std_dev)\n",
    "         \n",
    "# Add to dataframe\n",
    "tweets_df['Sentiment Scores'] = all_sentiment_scores\n",
    "tweets_df['Average'] = avg_scores\n",
    "tweets_df['Median'] = mdn_scores\n",
    "tweets_df['Range'] = rng_scores\n",
    "tweets_df['Variance'] = var_scores\n",
    "tweets_df['Standard Deviation'] = std_dev_scores\n",
    "\n",
    "# Export new file\n",
    "tweets_df.to_csv('tweet_sentiment_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: 1 >= score > 0    Vote YES on bill\n",
    "#      0 >= score >= -1  Vote NO on bill\n",
    "def bill_sentiment(tweet, bill):\n",
    "    if bill == \"Save the Internet\":\n",
    "        if ('#savetheinternet' in tweet) or ('#netneutrality' in tweet) or ('neutrality' in tweet):\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "               \n",
    "    elif bill == \"Health Care\":\n",
    "        if ('#medicareforall' in tweet) or ('#universalhealthcare' in tweet):\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "        \n",
    "    elif bill == \"Paycheck Fairness Act\":\n",
    "        if 'genderinequality' in tweet:\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "        \n",
    "    elif bill == \"For the People Act 2019\":\n",
    "        if ('#votingrights' in tweet) or ('#riggedelections' in tweet) or ('#forthepeople' in tweet):\n",
    "            return 1.0\n",
    "   \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "        \n",
    "    elif bill == \"Background Check for Firearms\":\n",
    "        if ('#guncontrol' in tweet) or ('#gunviolence' in tweet) or ('gunreform' in tweet):\n",
    "            return 1.0\n",
    "        if ('#gunrights' in tweet) or  ('noguncontrol' in tweet):\n",
    "            return -1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return score\n",
    "        \n",
    "    \n",
    "    elif bill == \"Remove Troops from Yemen\":\n",
    "        if '#yemengenocide' in tweet:\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "    \n",
    "    elif bill == \"Violence against Women Reauthorization Act\":\n",
    "        if ('#violenceagainstwomen' in tweet) or ('#genderbasedviolence' in tweet) or ('#sexualviolence' in tweet):\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "    \n",
    "    elif bill == \"Trump's National Emergency\":\n",
    "        if  '#trumpswall' in tweet:\n",
    "            return 1.0\n",
    "        \n",
    "        if ('#buildthewall' in tweet) or ('#buildthewallnow' in tweet) or ('#bordercrises' in tweet) or ('#illegalimmigration' in tweet) or ('illegal' in tweet):\n",
    "            return -1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "              \n",
    "    else:\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include cells that describe the steps in your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your ethics & privacy discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your discussion information here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
