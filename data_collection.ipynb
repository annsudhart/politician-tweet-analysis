{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your overview here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Mariam Issa\n",
    "- Andrea Sudharta\n",
    "- Payam Sadeghian\n",
    "- Brandon Amaral\n",
    "- Alex Luo\n",
    "- Jun Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members IDs\n",
    "\n",
    "- A12285140\n",
    "- A14497101\n",
    "- A13654507\n",
    "- A########\n",
    "- A########\n",
    "- A15743932"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How consistent are Congressmen/Congresswomen with the beliefs they state on Twitter and how they vote on various issues in Congress?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US government relies on its congress to check the powers of the president and other branches of government. One would expect to see nothing but honesty in this crucial cog of our government. It is shocking that members of Congress are some of the least trusted professionals. As Americans we assume those elected to serve us would want to preserve their reputation for honesty, but many people understand this isn't the case. Has lying become the norm for so long, that people have stopped caring. The Truther Tweet Team will uncover the reality of our government practices. In this project, we will analyze data to understand voting patterns and honesty in politicians and aim to uncover the deceivers in our congress. We will scan the Twitter accounts of congress members taking key words and hashtags for hot topics and compare their word against their voting patterns. Finally we will use statistical tests to decipher which members are significantly dishonest and those who aren’t.\n",
    "\t Over the years the public’s primary source of information has switched from Newspaper to Television and now to the internet. Specifically, politicians have taken a liking to the social media platform, Twitter to spread proposals. The hashtag tool, the like, and retweet features have proven effective to spread information for members with the same ideological standing. Essentially the political left and the political right have split into two groups on social media. This may result in people not checking the information they see or what those they follow vote. They mostly attack each other and think less of the other group. This environment is perfect for lying politicians to gain support from the average member of the political party. We predict some members are greater liars than others. In this study we aim to uncover those who tend to lie, a lot.\n",
    "\tIn other projects, those mainly research Trump’s impact on politics. The president is known as an excessive liar. Since Trumps’ introduction into politics, political analysts have seen a rise in lying accompanied by a defense of ‘fake news’ and ‘alternative facts’ against said lies. Other politicians have taken to such practices and we have one of the greatest problems in modern politics, excessive lying. “The Trump regime can be seen as post truth and hyper-Orwellian in its use of blatant lies, propaganda, and pure bullshit”. We can conclude that Trump has created an environment for lying politicians to grow.\n",
    "\n",
    "\n",
    "References:\n",
    "- 1)  https://www.forbes.com/sites/niallmccarthy/2019/01/11/americas-most-least-trusted-professions-infographic/#195b387c7e94\n",
    "- 2) https://link.springer.com/chapter/10.1007/978-981-10-8013-5_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: Both parties have the same consistency scores under 90% significant level. We predict that members of Congress will be mostly consistent between their tweets and their voting behavior, because of the easily accessible information that is available to the public (such as their voting ballots on GovTrack.us).  Coupling this watchdog effect, is the vast number of  media sources that serve to broadcast any unethical behavior from congress members, such as misleading and lying to their constituents on a platform such as Twitter.  The reason we are not predicting a 100% consistency in voting behaviors and Tweets is due to a lack of attentiveness of the American public in regards to how their representative votes, despite the easily-accessible information. Additionally, these past decades have witnessed a disappearance of investigative journalism, which means there may be a possibility of congress members voting with misaligned incentives due to low-risk repercussions.  This is an assumption that we would like to study in this project, because the extent of which this assumption is true will expose how honest congress members are to their constituents and to the general public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your dataset information here*\n",
    "\n",
    "(Copy this information for each dataset)\n",
    "- Dataset Name:\n",
    "- Link to the dataset:\n",
    "- Number of observations:\n",
    "\n",
    "1-2 sentences describing each dataset. \n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeters_dem_df = pd.read_csv('popular-tweeters-dem.csv') #Dataframe for list of Democratic Party politicians\n",
    "tweeters_rep_df = pd.read_csv('popular-tweeters-rep.csv') #Dataframe for list of Republican Party politicians\n",
    "tweeters_dem_df['Party'] = 'D'\n",
    "tweeters_rep_df['Party'] = 'R'\n",
    "tweeters_df = pd.concat([tweeters_dem_df, tweeters_rep_df], ignore_index = True) # Dataframe for combined list of politicians\n",
    "\n",
    "tweeters_handle = list(tweeters_df['Twitter_Handle']) #Python list for politician Twitter handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OAuth from Twitter\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'bSZRBubFkHWewMi08ltv7DgAu'\n",
    "consumer_secret = '6JFpZu66GT7OvoJFFxSfJJlX21NH5wxmMjNDbsQGPLyW9WtBSU'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "try:\n",
    "    redirect_url = auth.get_authorization_url()\n",
    "    print (redirect_url)\n",
    "    \n",
    "except tweepy.TweepError:\n",
    "    print(\"Error! Failed to get request token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Twitter account usage\n",
    "verifier = raw_input('Verifier:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Access Token\n",
    "try: \n",
    "    auth.get_access_token(verifier)\n",
    "except tweepy.TweepError:\n",
    "    print (\"Error! Failed to get access token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokens\n",
    "access_token = auth.access_token\n",
    "acess_token_secret = auth.access_token_secret\n",
    "\n",
    "# Note: You do not need to re-fetch it each time. Twitter currently does\n",
    "# not expire the tokens, so the only time it would ever go invalid is if\n",
    "# the user revokes our application access. \n",
    "\n",
    "#To rebuild an OAuthHandler from the stored access token\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(key, secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Set Up\n",
    "api = tweepy.API(auth)\n",
    "#api.update_status('tweepy + oauth!') # Posts directly on my timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tweets to only include those containing key words\n",
    "key_words_by_bill = {\n",
    "    0:['#savetheinternet', '#netneutrality', 'internet', 'neutrality'], \n",
    "    1:['#medicareforall', '#universalhealthcare','#healthcare', 'health', 'care', 'health', 'care', 'medicare'], \n",
    "    2: ['#equalpay', '#equalrights', '#equalrightsammendment', '#era', '#women', '#genderinequality', '#genderequality', 'equal', 'rights', 'pay', 'women', 'gender', 'equality'],\n",
    "    3:['#votingrights', '#votingrightact', '#electionintegrity', '#riggedelections', '#forthepeople', 'voting', 'rights', 'elections', 'fair', 'vote', 'votes'], \n",
    "    4:['#guncontrol', '#2ndammendment', '#noguncontol', '#gunviolence', '#gunrights', '#gunreform', '#2a', 'gun', 'second', '2nd',  'amendment'], \n",
    "    5:['#yemen', '#yemengenocide', 'yemen', 'yemeni', 'war', 'troops', 'withdraw troops', 'civil'], \n",
    "    6:['#women', '#reauthorizationact', '#violenceagainstwomen', '#genderbasedviolence', '#sexualviolence', 'women'],  \n",
    "    7:['#buildthewall', '#buildthewallnow', '#nationalsecurity', '#nationalemergency', '#trumpswall', '#illegalimmigration', '#mexico', '#border', '#bordercrises', 'mexico', 'illegal', 'immigration', 'security', 'emergency', 'build', 'wall']\n",
    "}\n",
    "\n",
    "key_words = list({x for v in key_words_by_bill.itervalues() for x in v})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Intersection Function\n",
    "def interSection(arr1,arr2): \n",
    "    return list(filter(lambda x: x in arr1, arr2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tweets that deal with each bill in Congress\n",
    "bill = [{}] * 8 # bill[bill_number] = {congressman: [tweets about bill]}\n",
    "tweets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_title = {\n",
    "    0: \"Save the Internet\",\n",
    "    1: \"Health Care\",\n",
    "    2: \"Paycheck Fairness Act\",\n",
    "    3: \"For the People Act 2019\",\n",
    "    4: \"Background Check for Firearms\",\n",
    "    5: \"Remove Troops from Yemen\",\n",
    "    6: \"Violence against Women Reauthorization Act\",\n",
    "    7: \"Trump's National Emergency\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through Congressmen\n",
    "for handle in tweeters_handle:\n",
    "    all_tweets = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            tweets_from_page_i = api.user_timeline(handle, count = 200, page = i) # max count value is 200\n",
    "        except:\n",
    "            print(\"TweepError: stopped at handle: \", handle, \" at page \", i)\n",
    "        \n",
    "        # Check if we reached end of Twitter history\n",
    "        if len(tweets_from_page_i) == 0: \n",
    "            break\n",
    "            \n",
    "        #iterate through each tweet, sort by bill\n",
    "        for tweet in tweets_from_page_i:\n",
    "            tweet_tokens = (tweet.text).lower().split()\n",
    "            \n",
    "            # See if Tweet talks about bill matter\n",
    "            for bill_number, bill_tokens in key_words_by_bill.items():\n",
    "                if len(interSection(tweet_tokens, bill_tokens)) != 0:\n",
    "                    if bill[bill_number] == {}:\n",
    "                        bill[bill_number] = {handle:[tweet.text]}\n",
    "                    else:\n",
    "                        if handle not in bill[bill_number]:\n",
    "                            bill[bill_number][handle] = [tweet.text]\n",
    "                        else:\n",
    "                            bill[bill_number][handle] += [tweet.text]\n",
    "                    \n",
    "                    # Compile to all tweets list\n",
    "                    all_tweets.append(tweet.text)\n",
    "                    \n",
    "    # Collect congressman/woman's tweets         \n",
    "    tweets[handle] = all_tweets\n",
    "        \n",
    "    # Sleep: to not overload Twitter requests\n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create look up table for Congressman/woman's name and party\n",
    "tweeters_party = list(tweeters_df['Party'])\n",
    "tweeters_name = list(tweeters_df['Name'])\n",
    "\n",
    "handle_to_party = {}\n",
    "handle_to_name = {}\n",
    "for i, handle in enumerate(tweeters_handle):\n",
    "    handle_to_party[handle] = tweeters_party[i]\n",
    "    handle_to_name[handle] = tweeters_name[i]\n",
    "\n",
    "data = [] \n",
    "\n",
    "# List all tweets by bill it's associated with and the congressman who tweeted it\n",
    "for i in range(len(bill)):\n",
    "    for handle, tweet in bill[i].items():\n",
    "        data.append([handle_to_name[handle], handle_to_party[handle], handle, bill_title[i], tweet])\n",
    "        \n",
    "tweeters_df = pd.DataFrame(data, columns = ['Name', 'Party', 'Twitter Handle', 'Bill #', 'Tweet'])\n",
    "\n",
    "# Export CSV with Tweets\n",
    "tweeters_df.to_csv('tweets_by_bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the Twitter dataset in the duration of this project, which meant that a number of the cleaning steps were carried out in the creation of the dataset. Since we chose to answer our data science question by selecting eight bills that were voted on in the 116th Congress, we filtered each representative’s Twitter history by only selecting Tweets that discerned one of the eight bills.  This was executed by checking whether or not the Tweet contained key words from the bill or a particular hashtag, demonstrating support for or against the bill. The last step taken in cleaning this data was the removal of any non-English words in the Tweet, which was due to the Tweepy library failing to remove all HTML code in the text part of the tweet. Python’s string function: isalpha() was used to remove these invalid parts of the Tweet text.  \n",
    "\n",
    "As for the eight GovTrack CSV files, which contained the votes on all eight bills, the cleaning stage of this dataset involved dropping all rows of congress people who were not selected for analysis. After this first step was taken, the eight CSV files were then merged into a single CSV file, where each row contained a representative and a column corresponding to their party, state, and their votes on the eight bills.  In this step, concern was raised on whether these cleaning steps were properly done since many representatives did not vote on any of the bills. After careful inspection, it was deducted that many representatives were not voting on any of these bills, which is particularly interesting since the congress people were selected by how active and popular they are on Twitter.  \n",
    "\n",
    "With these two datasets cleaned, the next step involved pre-processing the tweets into a numerical representation so that they can be directly compared to their voting pattern, which took on a value of ‘Yay, ‘Nay’, or ‘NA’ (no vote).  In order to categorize the tweets into the main two categories, we had to take a hybrid approach on how the text sentiment was conducted.  Detailed in the bill_sentiment() block of code (which was the function responsible for the text sentiment analysis), each bill contained key words or hashtags that inarguably reveals that politician’s sentiment  on the bill e.g. #trumpswall, which shows clear support for the bill of condemning Trump’s National Emergency to fund building the Mexican-American wall and the hashtag: #illegalimmigration, which supports Trump’s National Emergency, indicating a vote against the bill.  After this initial analysis, further sentiment analysis was carried out by utilizing Google’s Natural Language Processing Text Sentiment Analysis Cloud Tool to categorize the data. This step was particularly difficult to do since the NLP tool could only estimate the positivity and negativity of a tweet and since many congressmen’s tweets on Twitter are of a condescending tone, discerning their view on the topic was beyond the scope of analysis that could be done.  \n",
    "\n",
    "This was a critical limitation on our project because deducing the sentiment in regards to whether or not they approve or disapprove of a bill is a Natural Language Processing Task that leading Deep Learning Researchers are conducting their research on.  Despite this, the Google NLP tool was still used to get the sentiment score, a number between -1 (negative) and 1 (positive) and depending on the bill, this number was multiplied by -1 to capture whether the congress person would vote for or against a bill (this was done to standardize the sentiment and align the sentiment score with the predicted vote on a bill). After this step, the list of tweet texts, pertaining to each bill, became a list of sentiment scores, which was then averaged into a single number.  Finally, this score was converted into a 1 if it fell in the range [0.1, 1.0] to indicate support for the bill, a 0 if the score fell in the range [-0.1, 0.1] for a NA (no vote), or -1 if it fell between [-1.0, -0.1], as a vote against the bill.  After these cleaning steps, the dataset was set up to do a simple comparison between their predicted Tweet sentiment score and their recorded vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/mariamissa/Downloads/politician-tweet-sentiment-4910a807f774.json\"\n",
    "\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in a string\n",
    "def sentimentAnalysis(text):\n",
    "    document = types.Document(content=text, type=enums.Document.Type.PLAIN_TEXT)\n",
    "    \n",
    "    # Detects the sentiment of the text\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    \n",
    "    return sentiment.score, sentiment.magnitude\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweets_df = pd.read_csv('tweets_by_bill')\n",
    "\n",
    "all_sentiment_scores = []\n",
    "avg_scores = []\n",
    "mdn_scores = []\n",
    "rng_scores = [] \n",
    "var_scores = [] \n",
    "std_dev_scores = []\n",
    "\n",
    "for bill, tweets in zip(tweets_df['Bill #'], tweets_df['Tweet']):\n",
    "    \n",
    "    tweet_scores = []\n",
    "    \n",
    "    for twt in tweets.split('u\\''): \n",
    "        # Clean tweet\n",
    "        twt = ' '.join([word.lower() for word in twt.split(' ') if word.isalpha()])\n",
    "        \n",
    "        # Skip empty strings\n",
    "        if twt == '': continue \n",
    "            \n",
    "        # To not overload Google API\n",
    "        time.sleep(1)\n",
    "            \n",
    "        # Evaluate by bill\n",
    "        tweet_scores.append(bill_sentiment(twt, bill))\n",
    "    \n",
    "        \n",
    "    # Add list of scores to Dataframe\n",
    "    all_sentiment_scores.append(tweet_scores)\n",
    "    \n",
    "    # Get AVERAGE tweet sentiment score\n",
    "    avg = sum(tweet_scores) * 1.0 / len(tweet_scores)\n",
    "    avg_scores.append(avg)\n",
    "        \n",
    "    # Get MEDIAN tweet sentiment score\n",
    "    sorted_tweet_scores = sorted(tweet_scores)\n",
    "    mdn = sorted_tweet_scores[len(tweet_scores) / 2]\n",
    "    mdn_scores.append(mdn)\n",
    "        \n",
    "    # Get RANGE tweet sentiment score\n",
    "    rng = sorted_tweet_scores[len(tweet_scores) - 1] - sorted_tweet_scores[0]\n",
    "    rng_scores.append(rng)\n",
    "        \n",
    "    # Get VARIANCE tweet sentiment score\n",
    "    if len (tweet_scores) <= 1:\n",
    "        var = sum([(score - avg)**2 for score in tweet_scores]) \n",
    "    else:\n",
    "        var = sum([(score - avg)**2 for score in tweet_scores]) * 1.0 / (len(tweet_scores) - 1)\n",
    "    var_scores.append(var)\n",
    "    \n",
    "    # Get STD DEVIATION tweet sentiment score\n",
    "    std_dev = var**(1/2)\n",
    "    std_dev_scores.append(std_dev)\n",
    "         \n",
    "# Add to dataframe\n",
    "tweets_df['Sentiment Scores'] = all_sentiment_scores\n",
    "tweets_df['Average'] = avg_scores\n",
    "tweets_df['Median'] = mdn_scores\n",
    "tweets_df['Range'] = rng_scores\n",
    "tweets_df['Variance'] = var_scores\n",
    "tweets_df['Standard Deviation'] = std_dev_scores\n",
    "\n",
    "# Export new file\n",
    "tweets_df.to_csv('tweet_sentiment_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: 1 >= score > 0    Vote YES on bill\n",
    "#      0 >= score >= -1  Vote NO on bill\n",
    "def bill_sentiment(tweet, bill):\n",
    "    if bill == \"Save the Internet\":\n",
    "        if ('#savetheinternet' in tweet) or ('#netneutrality' in tweet) or ('neutrality' in tweet):\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "               \n",
    "    elif bill == \"Health Care\":\n",
    "        if ('#medicareforall' in tweet) or ('#universalhealthcare' in tweet):\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "        \n",
    "    elif bill == \"Paycheck Fairness Act\":\n",
    "        if 'genderinequality' in tweet:\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "        \n",
    "    elif bill == \"For the People Act 2019\":\n",
    "        if ('#votingrights' in tweet) or ('#riggedelections' in tweet) or ('#forthepeople' in tweet):\n",
    "            return 1.0\n",
    "   \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "        \n",
    "    elif bill == \"Background Check for Firearms\":\n",
    "        if ('#guncontrol' in tweet) or ('#gunviolence' in tweet) or ('gunreform' in tweet):\n",
    "            return 1.0\n",
    "        if ('#gunrights' in tweet) or  ('noguncontrol' in tweet):\n",
    "            return -1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return score\n",
    "        \n",
    "    \n",
    "    elif bill == \"Remove Troops from Yemen\":\n",
    "        if '#yemengenocide' in tweet:\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "    \n",
    "    elif bill == \"Violence against Women Reauthorization Act\":\n",
    "        if ('#violenceagainstwomen' in tweet) or ('#genderbasedviolence' in tweet) or ('#sexualviolence' in tweet):\n",
    "            return 1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "    \n",
    "    elif bill == \"Trump's National Emergency\":\n",
    "        if  '#trumpswall' in tweet:\n",
    "            return 1.0\n",
    "        \n",
    "        if ('#buildthewall' in tweet) or ('#buildthewallnow' in tweet) or ('#bordercrises' in tweet) or ('#illegalimmigration' in tweet) or ('illegal' in tweet):\n",
    "            return -1.0\n",
    "        \n",
    "        score, mag = sentimentAnalysis(tweet)\n",
    "        \n",
    "        return -1 * score\n",
    "              \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include cells that describe the steps in your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to web scrape Twitter for the tweet history of each congressman/woman, a Twitter Developer Account had to be requested from the company. Once this step was completed, we had the permission to collect all the Twitter data we needed to formulate the dataset. As for the GovTrack datasets, the CSV files, containing the votes on a bill from each House of Representatives, are public records (since the transparency of representatives voting record is vital for maintaining a representative democracy) no permission was needed.  \n",
    "\n",
    "In terms of privacy, this was not an issue for the GovTrack files since the CSV files are online and downloadable with no usage restrictions.  As for using the Twitter API and the tweepy library, there was a restriction on the number of page requests that could be made in an hourly basis. Thus, curating the dataset required the sleep function to be used to stay within Twitter’s allowance of web scraping.\n",
    "\n",
    "Working on this type of problem involved various challenges in terms of answering our data science question without bias. To best represent the population we are studying, the following steps were completed: an even number of representatives were selected from both the Democratic and Republican party; a systematic way of selecting which of the 435 politicians from the House of Representatives to use for analysis was specified (those with the most tweets and followers).  As for selecting which of the bills voted on in the House were used for analysis: the bills that had the most divide were used i.e. there was a near even split of votes.  This method of selecting bills was advantageous since it avoided biasedly selecting bills introduced to the house disproportionately by one party.  However, there was a significant issue in terms of how to filter the Tweet data.  The list of key words belonging to each bill were accumulated by taking key words in each of the bill statements and from analyzing the most popular results from searching each of the bills on Twitter. Whether or not Twitter has algorithm bias in their search results was out of the project’s scope of control; however, selecting the hashtags from these search results left room for human bias to unintendedly select a biased list of words and hashtags.  This was a challenge to the project since the poor execution of the step would skew the data set by disproportionately containing tweets predominantly from one side of the vote. However, since this was an issue raised before curating this list of key words, emphasis was places on analyzing tweets from both parties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in your discussion information here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
